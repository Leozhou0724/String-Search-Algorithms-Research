String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.String-searching algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.

Let £U be an alphabet (finite set). The most basic example of string searching is where both the pattern and searched text are arrays of elements of £U. The £U may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (£U = {0,1}) or DNA alphabet (£U = {A,C,G,T}) in bioinformatics.

In practice, how the string is encoded can affect the feasible string-search algorithms. In particular, if a variable-width encoding is in use then it may be slower to find the Nth character (perhaps requiring time proportional to N). This may significantly slow down some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]


Contents
1	Kinds of searching
2	Basic classification of search algorithms
2.1	Single-pattern algorithms
2.2	Algorithms using a finite set of patterns
2.3	Algorithms using an infinite number of patterns
3	Other classification
3.1	Naive string search
3.2	Finite-state-automaton-based search
3.3	Stubs
3.4	Index methods
3.5	Other variants
4	See also
5	References
6	External links
Kinds of searching
The most basic case of string searching involves one (often very long) string, sometimes called the "haystack", and one (often very short) string, sometimes called the "needle". The goal is to find one or more occurrences of the "needle" within the "haystack". For example, one might search for "to" within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.
One might request the first occurrence, which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.

Very commonly, however, various constraints are added. For example, one might want to match "needle" only where it consists of one (or more) complete words¡Xperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.

Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.

Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r
where the "?" conventionally makes the preceding character ("u") optional.

This article mainly discusses algorithms for the simpler kinds of string searching.

A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Basic classification of search algorithms
The various algorithms can be classified by the number of patterns each uses.

Single-pattern algorithms
Let m be the length of the pattern, n be the length of the searchable text and k = |£U| be the size of the alphabet.

Algorithm	Preprocessing time	Matching time[1]	Space
Naive string-search algorithm	none	£K(nm)	none
Rabin¡VKarp algorithm	£K(m)	average £K(n + m),
worst £K((n?m)m)	O(1)
Knuth¡VMorris¡VPratt algorithm	£K(m)	£K(n)	£K(m)
Boyer¡VMoore string-search algorithm	£K(m + k)	best £[(n/m),
worst O(mn)	£K(k)
Bitap algorithm (shift-or, shift-and, Baeza¡VYates¡VGonnet)	£K(m + k)	O(mn)	
Two-way string-matching algorithm	£K(m)	O(n+m)	O(1)
BNDM (Backward Non-Deterministic Dawg Matching)	O(m)	O(n)	
BOM (Backward Oracle Matching)	O(m)	O(mn)	
1.^ Asymptotic times are expressed using O, £[, and £K notation.
The Boyer¡VMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[3]

Algorithms using a finite set of patterns
Aho¡VCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
Rabin¡VKarp string search algorithm
Algorithms using an infinite number of patterns
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Other classification
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.

Classes of string searching algorithms[4]
Text not preprocessed	Text preprocessed
Patterns not preprocessed	Elementary algorithms	Index methods
Patterns preprocessed	Constructed search engines	Signature methods :[5]
Another one classifies the algorithms by their matching strategy:[6]

Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick)
Match the suffix first (Boyer-Moore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (Naive, Rabin-Karp)
Naive string search
A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search
DFA search mommy.svg
In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct¡Xthey are usually created using the powerset construction¡Xbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs
Knuth¡VMorris¡VPratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer¡VMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza¡VYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of Baeza¡VYates' approach.

Index methods
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in {\displaystyle \Theta (n)} \Theta (n) time, and all {\displaystyle z} z occurrences of a pattern can be found in {\displaystyle O(m)} O(m) time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Knuth–Morris–Pratt algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Knuth–Morris–Pratt algorithm" – news · newspapers · books · scholar · JSTOR (October 2009) (Learn how and when to remove this template message)
In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.

The algorithm was conceived in 1970 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. This was the first linear-time algorithm for string matching. The three published it jointly in 1977.[1] Independently, in 1969, Matiyasevich[2][3] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem.


Contents
1	Background
2	KMP algorithm
2.1	Example of the search algorithm
2.2	Description of pseudocode for the search algorithm
2.3	Efficiency of the search algorithm
3	"Partial match" table (also known as "failure function")
3.1	Worked example of the table-building algorithm
3.2	Description of pseudocode for the table-building algorithm
3.3	Efficiency of the table-building algorithm
4	Efficiency of the KMP algorithm
5	Variants
6	References
7	External links
Background
A string-matching algorithm wants to find the starting index m in string S[] that matches the search word W[].

The most straightforward algorithm is to look for a character match at successive values of the index m, the position in the string being searched, i.e. S[m]. If the index m reaches the end of the string then there is no match, in which case the search is said to "fail". At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0]. If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i. The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i]. If all successive characters match in W at position m, then a match is found at that position in the search string.

Usually, the trial check will quickly reject the trial match. If the strings are uniformly distributed random letters, then the chance that characters match is 1 in 26. In most cases, the trial check will reject the match at the initial letter. The chance that the first two letters will match is 1 in 262 (1 in 676). So if the characters are random, then the expected complexity of searching string S[] of length k is on the order of k comparisons or O(k). The expected performance is very good. If S[] is 1 million characters and W[] is 1000 characters, then the string search should complete after about 1.04 million character comparisons.

That expected performance is not guaranteed. If the strings are not random, then checking a trial m may take many character comparisons. The worst case is if the two strings match in all but the last letter. Imagine that the string S[] consists of 1 million characters that are all A, and that the word W[] is 999 A characters terminating in a final B character. The simple string-matching algorithm will now examine 1000 characters at each trial position before rejecting the match and advancing the trial position. The simple string search example would now take about 1000 character comparisons times 1 million positions for 1 billion character comparisons. If the length of W[] is n, then the worst-case performance is O(k⋅n).

The KMP algorithm has a better worst-case performance than the straightforward algorithm. KMP spends a little time precomputing a table (on the order of the size of W[], O(n)), and then it uses that table to do an efficient search of the string in O(k).

The difference is that KMP makes use of previous match information that the straightforward algorithm does not. In the example above, when KMP sees a trial match fail on the 1000th character (i = 999) because S[m+999] ≠ W[999], it will increment m by 1, but it will know that the first 998 characters at the new position already match. KMP matched 999 A characters before discovering a mismatch at the 1000th character (position 999). Advancing the trial match position m by one throws away the first A, so KMP knows there are 998 A characters that match W[] and does not retest them; that is, KMP sets i to 998. KMP maintains its knowledge in the precomputed table and two state variables. When KMP discovers a mismatch, the table determines how much KMP will increase (variable m) and where it will resume testing (variable i).

KMP algorithm
Example of the search algorithm
To illustrate the algorithm's details, consider a (relatively artificial) run of the algorithm, where W = "ABCDABD" and S = "ABC ABCDAB ABCDABCDABDE". At any given time, the algorithm is in a state determined by two integers:

m, denoting the position within S where the prospective match for W begins,
i, denoting the index of the currently considered character in W.
In each step the algorithm compares S[m+i] with W[i] and increments i if they are equal. This is depicted, at the start of the run, like

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456
The algorithm compares successive characters of W to "parallel" characters of S, moving from one to the next by incrementing i if they match. However, in the fourth step S[3] = ' ' does not match W[3] = 'D'. Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 1 and 2 in S; hence, having checked all those characters previously (and knowing they matched the corresponding characters in W), there is no chance of finding the beginning of a match. Therefore, the algorithm sets m = 3 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:    ABCDABD
i:    0123456
This match fails at the initial character, so the algorithm sets m = 4 and i = 0

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456
Here, i increments through a nearly complete match "ABCDAB" until i = 6 giving a mismatch at W[6] and S[10]. However, just prior to the end of the current partial match, there was that substring "AB" that could be the beginning of a new match, so the algorithm must take this into consideration. As these characters match the two characters prior to the current position, those characters need not be checked again; the algorithm sets m = 8 (the start of the initial prefix) and i = 2 (signaling the first two characters match) and continues matching. Thus the algorithm not only omits previously matched characters of S (the "AB"), but also previously matched characters of W (the prefix "AB").

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456
This search at the new position fails immediately because W[2] (a 'C') does not match S[10] (a ' '). As in the first trial, the mismatch causes the algorithm to return to the beginning of W and begins searching at the mismatched character position of S: m = 10, reset i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:           ABCDABD
i:           0123456
The match at m=10 fails immediately, so the algorithm next tries m = 11 and i = 0.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456
Once again, the algorithm matches "ABCDAB", but the next character, 'C', does not match the final character 'D' of the word W. Reasoning as before, the algorithm sets m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.

             1         2  
m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456
This time the match is complete, and the first character of the match is S[15].

Description of pseudocode for the search algorithm
The above example contains all the elements of the algorithm. For the moment, we assume the existence of a "partial match" table T, described below, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch. The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. The following is a sample pseudocode implementation of the KMP search algorithm.


algorithm kmp_search:
    input:
        an array of characters, S (the text to be searched)
        an array of characters, W (the word sought)
    output:
        an array of integers, P (positions in S at which W is found)
        an integer, nP (number of positions)

    define variables:
        an integer, j ← 0 (the position of the current character in S)
        an integer, k ← 0 (the position of the current character in W)
        an array of integers, T (the table, computed elsewhere)

    let nP ← 0

    while j < length(S) do
        if W[k] = S[j] then
            let j ← j + 1
            let k ← k + 1
            if k = length(W) then
                (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
                let P[nP] ← j - k, nP ← nP + 1
                let k ← T[k] (T[length(W)] can't be -1)
        else
            let k ← T[k]
            if k < 0 then
                let j ← j + 1
                let k ← k + 1
Efficiency of the search algorithm
Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation. Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])]. In particular, the next possible match must occur at a higher index than m, so that T[i] < i.

This fact implies that the loop can execute at most 2n times, since at each iteration it executes one of the two branches in the loop. The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased. The second branch adds i - T[i] to m, and as we have seen, this is always a positive number. Thus the location m of the beginning of the current potential match is increased. At the same time, the second branch leaves m + i unchanged, for m gets i - T[i] added to it, and immediately after T[i] gets assigned as the new value of i, hence new_m + new_i = old_m + old_i - T[old_i] + T[old_i] = old_m + old_i. Now, the loop ends if m + i = n; therefore, each branch of the loop can be reached at most n times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.

Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).

Here is another way to think about the runtime: Let us say we begin to match W and S at position i and p. If W exists as a substring of S at p, then W[0..m] = S[p..p+m]. Upon success, that is, the word and the text matched at the positions (W[i] = S[p+i]), we increase i by 1. Upon failure, that is, the word and the text do not match at the positions (W[i] ≠ S[p+i]), the text pointer is kept still, while the word pointer is rolled back a certain amount (i = T[i], where T is the jump table), and we attempt to match W[T[i]] with S[p+i]. The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll back as much as we have progressed up to the failure. Then it is clear the runtime is 2n.

"Partial match" table (also known as "failure function")
The goal of the table is to allow the algorithm not to match any character of S more than once. The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position. In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match. Hence T[i] is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W[i - 1]. We use the convention that the empty string has length 0. Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed below.

Worked example of the table-building algorithm
We consider the example of W = "ABCDABD" first. We will see that it follows much the same pattern as the main search, and is efficient for similar reasons. We set T[0] = -1. To find T[1], we must discover a proper suffix of "A" which is also a prefix of pattern W. But there are no proper suffixes of "A", so we set T[1] = 0. To find T[2], we see that the substring W[0] - W[1] ("AB") has a proper suffix "B". However "B" is not a prefix of the pattern W. Therefore, we set T[2] = 0.

Continuing to T[3], we first check the proper suffix of length 1, and as in the previous case it fails. Should we also check longer suffixes? No, we now note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix (A proper prefix of a string is not equal to the string itself) and ending at W[2] with length 2 (the maximum possible); then its first character is also a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined did not occur as T[2] = 0 and not T[2] = 1. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (i.e. T[x] = m) and should not bother to check m+2, m+3, etc.

Therefore, we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.

We pass to the subsequent W[4], 'A'. The same logic shows that the longest substring we need to consider has length 1, and as in the previous case it fails since "D" is not a prefix of W. But instead of setting T[4] = 0, we can do better by noting that W[4] = W[0], and also that a look-up of T[4] implies the corresponding S character, S[m+4], was a mismatch and therefore S[m+4] ≠ 'A'. Thus there is no point in restarting the search at S[m+4]; we should begin 1 ahead. This means that we may shift pattern W by match length plus one character, so T[4] = -1.

Considering now the next character, W[5], which is 'B': though by inspection the longest substring would appear to be 'A', we still set T[5] = 0. The reasoning is similar to why T[4] = -1. W[5] itself extends the prefix match begun with W[4], and we can assume that the corresponding character in S, S[m+5] ≠ 'B'. So backtracking before W[5] is pointless, but S[m+5] may be 'A', hence T[5] = 0.

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5]. Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.

Therefore, we compile the following table:

i	0	1	2	3	4	5	6	7
W[i]	A	B	C	D	A	B	D	
T[i]	-1	0	0	0	-1	0	2	0
Another example:

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	C	
T[i]	-1	0	-1	1	-1	0	-1	3	2	0
Another example (slightly changed from the previous example):

i	0	1	2	3	4	5	6	7	8	9
W[i]	A	B	A	C	A	B	A	B	A	
T[i]	-1	0	-1	1	-1	0	-1	3	-1	3
Another more complicated example:

i	00	01	02	03	04	05	06	07	08	09	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24
W[i]	P	A	R	T	I	C	I	P	A	T	E		I	N		P	A	R	A	C	H	U	T	E	
T[i]	-1	0	0	0	0	0	0	-1	0	2	0	0	0	0	0	-1	0	0	3	0	0	0	0	0	0
Description of pseudocode for the table-building algorithm
The example above illustrates the general technique for assembling the table with a minimum of fuss. The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it. The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning. This necessitates some initialization code.

 algorithm kmp_table:
    input:
        an array of characters, W (the word to be analyzed)
        an array of integers, T (the table to be filled)
    output:
        nothing (but during operation, it populates the table)

    define variables:
        an integer, pos ← 1 (the current position we are computing in T)
        an integer, cnd ← 0 (the zero-based index in W of the next character of the current candidate substring)

    let T[0] ← -1

    while pos < length(W) do
        if W[pos] = W[cnd] then
            let T[pos] ← T[cnd]
        else
            let T[pos] ← cnd
            let cnd ← T[cnd] (to increase performance)
            while cnd >= 0 and W[pos] <> W[cnd] do
                let cnd ← T[cnd]
        let pos ← pos + 1, cnd ← cnd + 1

    let T[pos] ← cnd (only need when all word occurrences searched)
Efficiency of the table-building algorithm
The complexity of the table algorithm is O(k), where k is the length of W. As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(k) time, which will be done by simultaneously examining the quantities pos and pos - cnd. In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased. In the second branch, cnd is replaced by T[cnd], which we saw above is always strictly less than cnd, thus increasing pos - cnd. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = k, it must terminate after at most 2k iterations of the loop, since pos - cnd begins at 1. Therefore, the complexity of the table algorithm is O(k).

Efficiency of the KMP algorithm
Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).

These complexities are the same, no matter how many repetitive patterns are in W or S.

Variants
A real-time version of KMP can be implemented using a separate failure function table for each character in the alphabet. If a mismatch occurs on character {\displaystyle x} x in the text, the failure function table for character {\displaystyle x} x is consulted for the index {\displaystyle i} i in the pattern at which the mismatch took place. This will return the length of the longest substring ending at {\displaystyle i} i matching a prefix of the pattern, with the added condition that the character after the prefix is {\displaystyle x} x. With this restriction, character {\displaystyle x} x in the text need not be checked again in the next phase, and so only a constant number of operations are executed between the processing of each index of the text[citation needed]. This satisfies the real-time computing restriction.

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

Boyer–Moore string-search algorithm
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Boyer-Moore theorem prover, see Nqthm.
In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.


Contents
1	Definitions
2	Description
3	Shift Rules
3.1	The Bad Character Rule
3.1.1	Description
3.1.2	Preprocessing
3.2	The Good Suffix Rule
3.2.1	Description
3.2.2	Preprocessing
4	The Galil rule
5	Performance
6	Implementations
7	Variants
8	References
9	External links
Definitions
A	N	P	A	N	M	A	N	-
P	A	N	-	-	-	-	-	-
-	P	A	N	-	-	-	-	-
-	-	P	A	N	-	-	-	-
-	-	-	P	A	N	-	-	-
-	-	-	-	P	A	N	-	-
-	-	-	-	-	P	A	N	-
Alignments of pattern PAN to text ANPANMAN, from k=3 to k=8. A match occurs at k=5.
S[i] denotes the character at index i of string S, counting from 1.
S[i..j] denotes the substring of string S starting at index i and ending at j, inclusive.
A prefix of S is a substring S[1..i] for some i in range [1, n], where n is the length of S.
A suffix of S is a substring S[i..n] for some i in range [1, n], where n is the length of S.
The string to be searched for is called the pattern and is denoted by P. Its length is n.
The string being searched in is called the text and is denoted by T. Its length is m.
An alignment of P to T is an index k in T such that the last character of P is aligned with index k of T.
A match or occurrence of P occurs at an alignment if P is equivalent to T[(k-n+1)..k].
Description
The Boyer-Moore algorithm searches for occurrences of P in T by performing explicit character comparisons at different alignments. Instead of a brute-force search of all alignments (of which there are {\displaystyle m-n+1} m-n+1), Boyer-Moore uses information gained by preprocessing P to skip as many alignments as possible.

Previous to the introduction of this algorithm, the usual way to search within text was to examine each character of the text for the first character of the pattern. Once that was found the subsequent characters of the text would be compared to the characters of the pattern. If no match occurred then the text would again be checked character by character in an effort to find a match. Thus almost every character in the text needs to be examined.

The key insight in this algorithm is that if the end of the pattern is compared to the text, then jumps along the text can be made rather than checking every character of the text. The reason that this works is that in lining up the pattern against the text, the last character of the pattern is compared to the character in the text. If the characters do not match, there is no need to continue searching backwards along the text. If the character in the text does not match any of the characters in the pattern, then the next character in the text to check is located n characters farther along the text, where n is the length of the pattern. If the character in the text is in the pattern, then a partial shift of the pattern along the text is done to line up along the matching character and the process is repeated. Jumping along the text to make comparisons rather than checking every character in the text decreases the number of comparisons that have to be made, which is the key to the efficiency of the algorithm.

More formally, the algorithm begins at alignment {\displaystyle k=n} k=n, so the start of P is aligned with the start of T. Characters in P and T are then compared starting at index n in P and k in T, moving backward. The strings are matched from the end of P to the start of P. The comparisons continue until either the beginning of P is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The comparisons are performed again at the new alignment, and the process repeats until the alignment is shifted past the end of T, which means no further matches will be found.

The shift rules are implemented as constant-time table lookups, using tables generated during the preprocessing of P.

Shift Rules
A shift is calculated by applying two rules: the bad character rule and the good suffix rule. The actual shifting offset is the maximum of the shifts calculated by these rules.

The Bad Character Rule
Description
-	-	-	-	X	-	-	K	-	-	-
A	N	P	A	N	M	A	N	A	M	-
-	N	N	A	A	M	A	N	-	-	-
-	-	-	N	N	A	A	M	A	N	-
Demonstration of bad character rule with pattern NNAAMAN.
The bad-character rule considers the character in T at which the comparison process failed (assuming such a failure occurred). The next occurrence of that character to the left in P is found, and a shift which brings that occurrence in line with the mismatched occurrence in T is proposed. If the mismatched character does not occur to the left in P, a shift is proposed that moves the entirety of P past the point of mismatch.

Preprocessing
Methods vary on the exact form the table for the bad character rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character c in the alphabet and second by the index i in the pattern. This lookup will return the occurrence of c in P with the next-highest index {\displaystyle j<i} j<i or -1 if there is no such occurrence. The proposed shift will then be {\displaystyle i-j} i-j, with {\displaystyle O(1)} O(1) lookup time and {\displaystyle O(kn)} O(kn) space, assuming a finite alphabet of length k.

The Good Suffix Rule
Description
-	-	-	-	X	-	-	K	-	-	-	-	-
M	A	N	P	A	N	A	M	A	N	A	P	-
A	N	A	M	P	N	A	M	-	-	-	-	-
-	-	-	-	A	N	A	M	P	N	A	M	-
Demonstration of good suffix rule with pattern ANAMPNAM.
The good suffix rule is markedly more complex in both concept and implementation than the bad character rule. It is the reason comparisons begin at the end of the pattern rather than the start, and is formally stated thus:[3]

Suppose for a given alignment of P and T, a substring t of T matches a suffix of P, but a mismatch occurs at the next comparison to the left. Then find, if it exists, the right-most copy t' of t in P such that t' is not a suffix of P and the character to the left of t' in P differs from the character to the left of t in P. Shift P to the right so that substring t' in P aligns with substring t in T. If t' does not exist, then shift the left end of P past the left end of t in T by the least amount so that a prefix of the shifted pattern matches a suffix of t in T. If no such shift is possible, then shift P by n places to the right. If an occurrence of P is found, then shift P by the least amount so that a proper prefix of the shifted P matches a suffix of the occurrence of P in T. If no such shift is possible, then shift P by n places, that is, shift P past t.

Preprocessing
The good suffix rule requires two tables: one for use in the general case, and another for use when either the general case returns no meaningful result or a match occurs. These tables will be designated L and H respectively. Their definitions are as follows:[3]

For each i, {\displaystyle L[i]} L[i] is the largest position less than n such that string {\displaystyle P[i..n]} P[i..n] matches a suffix of {\displaystyle P[1..L[i]]} P[1..L[i]] and such that the character preceding that suffix is not equal to {\displaystyle P[i-1]} P[i-1]. {\displaystyle L[i]} L[i] is defined to be zero if there is no position satisfying the condition.

Let {\displaystyle H[i]} H[i] denote the length of the largest suffix of {\displaystyle P[i..n]} P[i..n] that is also a prefix of P, if one exists. If none exists, let {\displaystyle H[i]} H[i] be zero.

Both of these tables are constructible in {\displaystyle O(n)} O(n) time and use {\displaystyle O(n)} O(n) space. The alignment shift for index i in P is given by {\displaystyle n-L[i]} n-L[i] or {\displaystyle n-H[i]} n-H[i]. H should only be used if {\displaystyle L[i]} L[i] is zero or a match has been found.

The Galil rule
A simple but important optimization of Boyer-Moore was put forth by Galil in 1979.[4] As opposed to shifting, the Galil rule deals with speeding up the actual comparisons done at each alignment by skipping sections that are known to match. Suppose that at an alignment k1, P is compared with T down to character c of T. Then if P is shifted to k2 such that its left end is between c and k1, in the next comparison phase a prefix of P must match the substring T[(k2 - n)..k1]. Thus if the comparisons get down to position k1 of T, an occurrence of P can be recorded without explicitly comparing past k1. In addition to increasing the efficiency of Boyer-Moore, the Galil rule is required for proving linear-time execution in the worst case.

Performance
The Boyer-Moore algorithm as presented in the original paper has worst-case running time of {\displaystyle O(n+m)} O(n+m) only if the pattern does not appear in the text. This was first proved by Knuth, Morris, and Pratt in 1977,[5] followed by Guibas and Odlyzko in 1980[6] with an upper bound of 5n comparisons in the worst case. Richard Cole gave a proof with an upper bound of 3n comparisons in the worst case in 1991.[7]

When the pattern does occur in the text, running time of the original algorithm is {\displaystyle O(nm)} O(nm) in the worst case. This is easy to see when both pattern and text consist solely of the same repeated character. However, inclusion of the Galil rule results in linear runtime across all cases.[4][7]

Implementations
Various implementations exist in different programming languages. In C++, Boost provides the generic Boyer–Moore search implementation under the Algorithm library. In Go (programming language) there is an implementation in search.go. D (programming language) uses a BoyerMooreFinder for predicate based matching within ranges as a part of the Phobos Runtime Library.

The Boyer-Moore algorithm is also used in GNU's grep.[8]

Below are a few simple implementations.